{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import Libraries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "## Import libraries\n",
    "\n",
    "import os, glob, pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string, re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numbers\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Input Raw Data & HISCO list **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter path to ads directory (without parentheses): C://Users//Ruben//Downloads//metadata//leco_set\n",
      "Enter filename of ads.csv (without parentheses): 1880-1890.csv\n",
      "Enter path to hisco text file (without parentheses): d\n",
      "Enter abbreviation of newspaper: d\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--csv_directory\", help = \"path to the csv file that contains all the ads (obtained through KB API)\")\n",
    "parser.add_argument(\"--hisco_directory\", help = \"path to the txt file with HISCO occupations\")\n",
    "parser.add_argument(\"--npp_name\", help = \"newspaper abbreviation, four characters (leco, nvdd,roni,nroc,algh,dzgh etc.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Raw Data (.csv file with harvested advertisements (using the KB-harvesting script))\n",
    "\n",
    "os.chdir(str(args.csv_directory))\n",
    "csvs = glob.glob('*.csv')\n",
    "df = pd.read_csv(csvs[0], sep = '\\t')\n",
    "\n",
    "# to extract a random sample of advertisements:\n",
    "#df = df.sample(n=20000)\n",
    "#df = df.reset_index(drop=True)\n",
    "\n",
    "# Import HISCO list\n",
    "os.chdir(str(args.hisco_directory))\n",
    "listtxt = glob.glob('*.txt')\n",
    "fn = [f for f in listtxt if \"occ_total\" in f][0]\n",
    "with open(fn, encoding = 'utf-8') as f:\n",
    "    list_words = f.read().splitlines()\n",
    "\n",
    "list_words = list(set(list_words))\n",
    "list_words = list_words + ['loopjongen']\n",
    "\n",
    "\n",
    "print(\"1. resources imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define Cleaning and Other Useful Functions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set cleaning (or tokenizing) function\n",
    "## This function removes punctuation using string + regex and splits sentences on spaces\n",
    "\n",
    "def clean_and_split_str(txt):\n",
    "    #strip_special_chars = re.compile(\"[^A-Za-z0-9#]+\")\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    txt = txt.translate(translator)\n",
    "    txt = re.sub('\\s+', ' ', txt).strip()\n",
    "    txt = txt.lower()\n",
    "    txt = txt.split(' ')\n",
    "    return txt\n",
    "\n",
    "## Function to detect whether a string contains a number\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data Tokenizing **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = [clean_and_split_str(i) for i in df.ocr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Line Extraction **\n",
    "\n",
    "The loop below searches for occupations in the cleaned articles. A list is used that includes the HISCO occupation and OCR-'alternatives' aquired through word embeddings and string distance measuring. Once an occupation is identified, a region 'around' it (L12:R40) is selected. This window is chosen because it seems to yield the most occupation-wage combinations. Larger windows to the right can be considered.\n",
    "\n",
    "Two choices in this step influence the outcome\n",
    "\n",
    "- the window. Lots of close reading more or less shows that the occupation - wage combi is found relatively close together, allowing for a relatively small window (< 50 to the right).\n",
    "\n",
    "- the word 'loon'. If this word is not in the extracted window or 'slice', the window is not included. Again, reading through lots of advertisements justifies this choice, because practically all the job advertisements contains the word 'loon'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11%\n",
      "22%\n",
      "32%\n",
      "43%\n",
      "54%\n",
      "65%\n",
      "75%\n",
      "86%\n",
      "97%\n"
     ]
    }
   ],
   "source": [
    "lo = list() \n",
    "ctr = 0 # counter for showing progress\n",
    "\n",
    "for c,i in enumerate(df.clean):\n",
    "    ctr += 1\n",
    "    \n",
    "    if any(e in i for e in list_words):     # check if there is an occupation in the advertisement\n",
    "        \n",
    "        occ = list()\n",
    "        \n",
    "        for w in i:                         # loop over every word and check if it is an advertisement (faster using list comprehension!!!!)\n",
    "            if w in list_words:\n",
    "                occ.append(w)\n",
    "        \n",
    "        for o in occ:                       # loop over occupations in ad-occupation list and select windows around 'em\n",
    "            ind = i.index(o)\n",
    "            sl = i[ind-12:ind+40]\n",
    "            sl = ' '.join(sl)\n",
    "            if 'loon ' in sl:               # if the word 'loon' is in the window; append [occupation:window] to list (not dictionary because of duplicate occupations)\n",
    "                lo.append([o,sl])\n",
    "        \n",
    "    if ctr % 10000 == 0:                    # print the progress\n",
    "        print(str(round(ctr / len(df.clean) * 100)) + \"%\")\n",
    "        \n",
    "print(\"2. lines extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Set Wage Extraction Function **\n",
    "\n",
    "In the function below, we specify how wages should be extracted. The function requires a string input (the windows extracted in the previous step). The steps are the following:\n",
    "\n",
    "- we split the string in a list of words (on spaces, because we tokenized earlier)\n",
    "- we identify the location of the word \"loon\"\n",
    "- we select a window around \"loon\" #windowinception\n",
    "- we extract the words in this loon-window that contain numbers (using the function defined earlier)\n",
    "- then, there are two options:\n",
    "    - the list of tokens-containing-numbers is empty: we either  look if indicative words such as 'goed' and 'hoog' are in the window-in-window and set the value of wage to \"qualitative\", or, when this is not the case we leave this advertisement to rott in the gutters of Delpher.\n",
    "    - the list of tokens-containing-numbers contains numbers: we identify the position of this number-containing-token and if it is one or two positions right of loon, it is set as the wage corresponding to the occupation. If it is farther away it is probably a random number or an OCR error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CloseRangeWage(string):\n",
    "    # Split string\n",
    "    s = string.split()\n",
    "    \n",
    "    # Check if we have separate f-sign, no f-sign or one f-sign combined with wages\n",
    "    \n",
    "    loon_instance = [x for x in s if \"loon\" in x]\n",
    "    loon_instance_index = s.index(loon_instance[0])\n",
    "    loon_proximity_string = s[loon_instance_index-3:loon_instance_index + 4]\n",
    "    \n",
    "    tokens_w_digits = list([token for token in loon_proximity_string if hasNumbers(token) == True])\n",
    "    \n",
    "    ## If there are no tokens with digits:\n",
    "    \n",
    "    if len(tokens_w_digits) == 0:\n",
    "        \n",
    "        list_qualitative_wage_indicators=['goed', 'hoog', 'behoorlijk', 'bekwaamheid', 'overeen', 'flink']\n",
    "\n",
    "        if any(x in loon_proximity_string for x in list_qualitative_wage_indicators):\n",
    "\n",
    "            qualitative_wage = list([i for i in loon_proximity_string if i in list_qualitative_wage_indicators])\n",
    "            if len(qualitative_wage) > 0:\n",
    "                loon = \"qualitative\"\n",
    "                                     \n",
    "        elif \"Æ’\" in s[loon_instance_index+1] or \"f\" in s[loon_instance_index+1]:\n",
    "            \n",
    "            if len(s[loon_instance_index+1]) == 1:\n",
    "                if len(s) <= loon_instance_index+2:\n",
    "                    loon = \"no wage indication\"\n",
    "                else: \n",
    "                    loon = s[loon_instance_index+2]\n",
    "            if len(s[loon_instance_index+1]) >= 1:\n",
    "                if len(s) < loon_instance_index+1:\n",
    "                    loon = \"no wage indication\"\n",
    "                else: \n",
    "                    loon = s[loon_instance_index+1]\n",
    "                    \n",
    "        elif any(x in loon_proximity_string for x in ['gulden']):\n",
    "        \n",
    "            gulden_wage = list([i for i in loon_proximity_string if i in ['gulden']])[0]\n",
    "            \n",
    "            index_gulden = s.index(gulden_wage)\n",
    "            \n",
    "            loon = s[index_gulden-1]\n",
    "            \n",
    "        else:    \n",
    "            loon = \"no wage indication\"\n",
    "        \n",
    "    ## If there are tokens with digits:\n",
    "    \n",
    "    for token in tokens_w_digits:\n",
    "        \n",
    "        # Check if token before token with digit is \"loon\"\n",
    "        \n",
    "        token_index = loon_proximity_string.index(token)\n",
    "        \n",
    "        if loon_proximity_string[token_index-1] == \"loon\" or loon_proximity_string[token_index-2] == \"loon\":\n",
    "            loon = token\n",
    "            \n",
    "        else:\n",
    "            loon = \"no wage indication\"\n",
    "            \n",
    "    return loon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define a Function that Converts the Number-Containing-Token to a quantifyable wage **\n",
    "\n",
    "The advertisement OCR is pretty bad. Many of the wages identified earlier are misinterpreted. Also, the currency sign ('f') is sometimes not separated from the wage itself. The function below converts the wage-string to a number and solves the most common OCR-errors (1 as l, 0 as o, f in wage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WageString2Number(string):\n",
    "\n",
    "    ## Check for f in beginning:\n",
    "    \n",
    "    if string[0] == \"f\" or string[0] == \"Æ’\":\n",
    "    \n",
    "        string = string.replace(\"Æ’\",\"\")\n",
    "        string = string.replace(\"f\",\"\")\n",
    "        \n",
    "    string = string.replace(\"l\", \"1\")\n",
    "    string = string.replace(\"o\", \"0\")\n",
    "    \n",
    "    try:\n",
    "        wage = int(string)\n",
    "        \n",
    "    \n",
    "    except ValueError:\n",
    "        wage = \"inconvertable\"\n",
    "        \n",
    "    return wage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Run the Functions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "\n",
    "for k,v in lo:\n",
    "    \n",
    "    res = CloseRangeWage(v)\n",
    "    \n",
    "    results.append([k,res])\n",
    "result_df = pd.DataFrame([[v for k,v in results], [k for k,v in results]]).T\n",
    "result_df.columns = ['res', 'occ']\n",
    "\n",
    "print(\"3. wages extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(str(args.csv_directory))\n",
    "fn = args.npp_name + \"_extracted_wages.csv\"\n",
    "result_df.to_csv(fn, index=False)\n",
    "print('extraced wages saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
